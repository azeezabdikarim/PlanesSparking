{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import math\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "\n",
    "findspark.init('/Users/AzeezA/opt/spark-3.2.0/')\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"planes\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "1988,1,9,6,1348,1331,1458,1435,PI,942,NA,70,64,NA,23,17,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,10,7,1334,1331,1443,1435,PI,942,NA,69,64,NA,8,3,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,11,1,1446,1331,1553,1435,PI,942,NA,67,64,NA,78,75,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,12,2,1334,1331,1438,1435,PI,942,NA,64,64,NA,3,3,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,13,3,1341,1331,1503,1435,PI,942,NA,82,64,NA,28,10,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,14,4,1332,1331,1447,1435,PI,942,NA,75,64,NA,12,1,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,15,5,1331,1331,1434,1435,PI,942,NA,63,64,NA,-1,0,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,16,6,1327,1331,1427,1435,PI,942,NA,60,64,NA,-8,-4,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1988,1,17,7,1331,1331,1440,1435,PI,942,NA,69,64,NA,5,0,SYR,BWI,273,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n"
     ]
    }
   ],
   "source": [
    "planes = sc.textFile(\"/Users/AzeezA/Google\\ Drive/Life/2021/UPM/Big\\ Data/Planes/1988.csv.bz2\")\n",
    "# planes = sc.textFile(\"hdfs://localhost:9000/temp/1988.csv.bz2\")  //if file exists in HDFS use this line\n",
    "\n",
    "\n",
    "print()\n",
    "ten_lines = planes.take(10)\n",
    "for l in ten_lines:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5202096, 29)\n"
     ]
    }
   ],
   "source": [
    "column_names = planes.take(1)[0].split(',')\n",
    "\n",
    "header = planes.first()\n",
    "planes_no_header = planes.filter(lambda line: line != header)\n",
    "\n",
    "split_planes = planes_no_header.map(lambda x : x.split(','))\n",
    "\n",
    "full_planes_df = spark.createDataFrame(split_planes, schema = column_names)\n",
    "print((full_planes_df.count(), len(full_planes_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Azeez# convert time format from HoursMinutes to Minutes ex. 1348 -> 828\n",
    "time_to_min_cols = ['DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime']\n",
    "for c in time_to_min_cols:\n",
    "    full_planes_df = full_planes_df.withColumn(c, \n",
    "                                   ((full_planes_df[c]/100).cast('int'))*60 + (full_planes_df[c]%100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'string'), ('Month', 'string'), ('DayofMonth', 'string'), ('DayOfWeek', 'string'), ('DepTime', 'double'), ('CRSDepTime', 'double'), ('ArrTime', 'double'), ('CRSArrTime', 'double'), ('UniqueCarrier', 'string'), ('FlightNum', 'string'), ('TailNum', 'string'), ('ActualElapsedTime', 'string'), ('CRSElapsedTime', 'string'), ('AirTime', 'string'), ('ArrDelay', 'string'), ('DepDelay', 'string'), ('Origin', 'string'), ('Dest', 'string'), ('Distance', 'string'), ('TaxiIn', 'string'), ('TaxiOut', 'string'), ('Cancelled', 'string'), ('CancellationCode', 'string'), ('Diverted', 'string'), ('CarrierDelay', 'string'), ('WeatherDelay', 'string'), ('NASDelay', 'string'), ('SecurityDelay', 'string'), ('LateAircraftDelay', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(full_planes_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original columns: 29\n",
      "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime', 'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
      "\n",
      "\n",
      "Now only 19 columns remain\n",
      "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'TailNum', 'CRSElapsedTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'TaxiOut', 'Cancelled', 'CancellationCode']\n",
      "CPU times: user 17.9 ms, sys: 5.45 ms, total: 23.3 ms\n",
      "Wall time: 405 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'Number of original columns: {len(full_planes_df.columns)}')\n",
    "print(full_planes_df.columns)\n",
    "\n",
    "excluded_cols = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted',\n",
    "                   'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n",
    "int_cols = ['Year', 'DayofMonth', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'FlightNum', \n",
    "            'TailNum', 'CRSElapsedTime', 'DepDelay', 'Distance', 'TaxiOut', 'Cancelled', 'CancellationCode', 'ArrDelay']\n",
    "cat_cols = ['UniqueCarrier', 'Orig', 'Dest']\n",
    "\n",
    "\n",
    "target_col = 'ArrDelay'\n",
    "\n",
    "planes_df = full_planes_df.drop(*excluded_cols)\n",
    "\n",
    "for i in int_cols:\n",
    "    planes_df = planes_df.withColumn(i , planes_df[i].cast(IntegerType()))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(f'Now only {len(planes_df.columns)} columns remain')\n",
    "print(planes_df.columns)\n",
    "\n",
    "full_planes_df.createOrReplaceTempView(\"FULL_DF\")\n",
    "planes_df.createOrReplaceTempView(\"FLIGHT_LIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circular Transformation (Angular Distance)\n",
    "The following cells transforms specific columns so that the circular nature of their value is preserved. For exmaple, when days of the week are encoded as 1-7 where 1=Monday and 7=Sunday, the difference between Monday and Sunday is falsly inflated. Baded on this encoding, we woudl say Monday(1) and Thursday (4) are close togehter, when this is not actually true. To account for this, we calculate the sin and cosine of these columns to create a 'circular' encoding. \n",
    "This trnasofrmation is applied to the Months column, DayOfWeek column, as well as the columns that represent a specific time of the day (DepTime, CRSDepTime, CRSArrTime). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------+------------------+--------------------+--------------------+\n",
      "|Month|DayOfWeek|          Month_sin|         Month_cos|       DayOfWeek_sin|       DayOfWeek_cos|\n",
      "+-----+---------+-------------------+------------------+--------------------+--------------------+\n",
      "|    1|        6|0.49999999999999994|0.8660254037844387| -0.7818314824680299|  0.6234898018587334|\n",
      "|    1|        7|0.49999999999999994|0.8660254037844387|-2.44929359829470...|                 1.0|\n",
      "|    1|        1|0.49999999999999994|0.8660254037844387|  0.7818314824680298|  0.6234898018587336|\n",
      "|    1|        2|0.49999999999999994|0.8660254037844387|  0.9749279121818236|-0.22252093395631434|\n",
      "|    1|        3|0.49999999999999994|0.8660254037844387| 0.43388373911755823|  -0.900968867902419|\n",
      "|    1|        4|0.49999999999999994|0.8660254037844387|  -0.433883739117558| -0.9009688679024191|\n",
      "|    1|        5|0.49999999999999994|0.8660254037844387| -0.9749279121818236| -0.2225209339563146|\n",
      "|    1|        6|0.49999999999999994|0.8660254037844387| -0.7818314824680299|  0.6234898018587334|\n",
      "|    1|        7|0.49999999999999994|0.8660254037844387|-2.44929359829470...|                 1.0|\n",
      "|    1|        1|0.49999999999999994|0.8660254037844387|  0.7818314824680298|  0.6234898018587336|\n",
      "+-----+---------+-------------------+------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "-RECORD 0--------------------------------\n",
      " Year             | 1988                 \n",
      " Month            | 1                    \n",
      " DayofMonth       | 9                    \n",
      " DayOfWeek        | 6                    \n",
      " DepTime          | 828                  \n",
      " CRSDepTime       | 811                  \n",
      " CRSArrTime       | 875                  \n",
      " UniqueCarrier    | PI                   \n",
      " FlightNum        | 942                  \n",
      " TailNum          | null                 \n",
      " CRSElapsedTime   | 64                   \n",
      " ArrDelay         | 23                   \n",
      " DepDelay         | 17                   \n",
      " Origin           | SYR                  \n",
      " Dest             | BWI                  \n",
      " Distance         | 273                  \n",
      " TaxiOut          | null                 \n",
      " Cancelled        | 0                    \n",
      " CancellationCode | null                 \n",
      " Month_sin        | 0.49999999999999994  \n",
      " Month_cos        | 0.8660254037844387   \n",
      " DayOfWeek_sin    | -0.7818314824680299  \n",
      " DayOfWeek_cos    | 0.6234898018587334   \n",
      " DepTime_sin      | -0.4539904997395467  \n",
      " DepTime_cos      | -0.8910065241883679  \n",
      " CRSDepTime_sin   | -0.38671096163682067 \n",
      " CRSDepTime_cos   | -0.9222009716704518  \n",
      " CRSArrTime_sin   | -0.6259234721840591  \n",
      " CRSArrTime_cos   | -0.7798844830928818  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sin, cos\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Transform Month and DayOfWeek Column to circular transformation so that Mon-Sun (1,7) is closer than Mon-Thurs (1,4)\n",
    "date_cols = ['Month', 'DayOfWeek']\n",
    "for dc in date_cols:\n",
    "    planes_df = planes_df.withColumn(dc, planes_df[dc].cast(IntegerType()))\n",
    "    if dc == 'Month':\n",
    "        planes_df = planes_df.withColumn(dc + '_sin', sin((2*np.pi/12)*planes_df[dc]))\n",
    "        planes_df = planes_df.withColumn(dc + '_cos', cos((2*np.pi/12)*planes_df[dc]))\n",
    "    elif dc == 'DayOfWeek':\n",
    "        planes_df = planes_df.withColumn(dc + '_sin', sin((2*np.pi/7)*planes_df[dc]))\n",
    "        planes_df = planes_df.withColumn(dc + '_cos', cos((2*np.pi/7)*planes_df[dc]))\n",
    "        \n",
    "time_cols = ['DepTime', 'CRSDepTime', 'CRSArrTime']\n",
    "mins_in_day = 24*60\n",
    "for tc in time_cols:\n",
    "    planes_df = planes_df.withColumn(tc, planes_df[tc].cast(IntegerType()))\n",
    "    planes_df = planes_df.withColumn(tc + '_sin', sin((2*np.pi/mins_in_day)*planes_df[tc]))\n",
    "    planes_df = planes_df.withColumn(tc + '_cos', cos((2*np.pi/mins_in_day)*planes_df[tc]))\n",
    "        \n",
    "# planes_df.select(date_cols).show(n=10)\n",
    "# planes_df.drop(*date_cols)\n",
    "planes_df.select(['Month', 'DayOfWeek','Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos']).show(n=10)\n",
    "planes_df.show(n=1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|ArrDelay|\n",
      "+--------------------+--------+\n",
      "|[0.49999999999999...|      23|\n",
      "|[0.49999999999999...|       8|\n",
      "|[0.49999999999999...|      78|\n",
      "|[0.49999999999999...|       3|\n",
      "|[0.49999999999999...|      28|\n",
      "|[0.49999999999999...|      12|\n",
      "|[0.49999999999999...|      -1|\n",
      "|[0.49999999999999...|      -8|\n",
      "|[0.49999999999999...|       5|\n",
      "|[0.49999999999999...|      44|\n",
      "+--------------------+--------+\n",
      "\n",
      "CPU times: user 45.3 ms, sys: 16.7 ms, total: 62 ms\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#no circular standardization of Month, Day, and time columns\n",
    "# training_cols = ['Month', 'DayOfWeek', 'DayofMonth', 'DepTime', 'CRSDepTime', 'CRSArrTime', 'CRSElapsedTime',\n",
    "#                 'DepDelay', 'Distance']\n",
    "training_cols = ['Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'DayofMonth', 'DepTime_sin', 'DepTime_cos', 'CRSDepTime_sin', 'CRSDepTime_cos', 'CRSArrTime_sin', 'CRSArrTime_cos', 'CRSElapsedTime',\n",
    "                'DepDelay', 'Distance']\n",
    "\n",
    "target_col = 'ArrDelay'\n",
    "\n",
    "\n",
    "features = planes_df.select(training_cols + [target_col] )\n",
    "outputCol = 'features'\n",
    "\n",
    "df_va = VectorAssembler(inputCols = training_cols, outputCol = outputCol, handleInvalid=\"skip\")\n",
    "features = df_va.transform(features)\n",
    "\n",
    "\n",
    "features = features.na.drop(subset = training_cols + [target_col, outputCol])\n",
    "\n",
    "model_data = features.select([outputCol, target_col])\n",
    "\n",
    "# # # Use scaler of choice; here Standard scaler is used\n",
    "scalar = StandardScaler(inputCol=\"features\", outputCol=\"scaled_vectored_features\")\n",
    "scaled_features = scalar.fit(model_data.select('features'))\n",
    "scaled_model_data = scaled_features.transform(model_data)\n",
    "\n",
    "\n",
    "model_data.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|ArrDelay|\n",
      "+--------------------+--------+\n",
      "|[0.70690757706162...|      23|\n",
      "|[0.70690757706162...|       8|\n",
      "|[0.70690757706162...|      78|\n",
      "|[0.70690757706162...|       3|\n",
      "|[0.70690757706162...|      28|\n",
      "|[0.70690757706162...|      12|\n",
      "|[0.70690757706162...|      -1|\n",
      "|[0.70690757706162...|      -8|\n",
      "|[0.70690757706162...|       5|\n",
      "|[0.70690757706162...|      44|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaled_model_data = scaled_model_data.withColumn('features', scaled_model_data['scaled_vectored_features'])\n",
    "scaled_model_data = scaled_model_data.drop('scaled_vectored_features')\n",
    "scaled_model_data.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([0.5, 0.866, -0.7818, 0.6235, 9.0, -0.454, -0.891, -0.3867, -0.9222, -0.6259, -0.7799, 64.0, 17.0, 273.0]), ArrDelay=23, scaled_vectored_features=DenseVector([0.7069, 1.2252, -1.1017, 0.8852, 1.0225, -0.6379, -1.5505, -0.5398, -1.6388, -1.01, -1.1644, 1.0381, 0.8206, 0.5446]))]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.ml.feature.StandardScalerModel'>\n"
     ]
    }
   ],
   "source": [
    "print(scaled_model_data.take(1))\n",
    "print(type(model_data))\n",
    "print(type(scaled_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with features not scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [0.5738143759813829,1.0049912250367061,0.2812057295831131,-1.0372128999973347,-0.010034714061350222,0.006149261310895182,-0.006608570549335716,0.0010592355870998505,-0.07909467272539676,0.8974097655567339,0.008462300271836594]\n",
      "Intercept:  3.318032071420901\n",
      "Coefficients:\n",
      "       Feature Estimate Std Error   T Value P Value\n",
      "   (Intercept)   3.3180    0.0300  110.6891  0.0000\n",
      "     Month_sin   0.5738    0.0094   61.1696  0.0000\n",
      "     Month_cos   1.0050    0.0094  107.0469  0.0000\n",
      " DayOfWeek_sin   0.2812    0.0093   30.0978  0.0000\n",
      " DayOfWeek_cos  -1.0372    0.0094 -110.1567  0.0000\n",
      "    DayofMonth  -0.0100    0.0008  -13.3223  0.0000\n",
      "       DepTime   0.0061    0.0001   61.7413  0.0000\n",
      "    CRSDepTime  -0.0066    0.0001  -66.2558  0.0000\n",
      "    CRSArrTime   0.0011    0.0000   26.6471  0.0000\n",
      "CRSElapsedTime  -0.0791    0.0005 -165.8729  0.0000\n",
      "      DepDelay   0.8974    0.0003 2748.3446  0.0000\n",
      "      Distance   0.0085    0.0001  144.4421  0.0000\n",
      "\n",
      "(Dispersion parameter for gaussian family taken to be 182.5351)\n",
      "   Null deviance: 2223900401.4166 on 4100761 degrees of freedom\n",
      "Residual deviance: 748532791.4544 on 4100761 degrees of freedom\n",
      "AIC: 32989992.6472\n",
      "CPU times: user 48.6 ms, sys: 21.8 ms, total: 70.3 ms\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "(train_data, test_data) = model_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "glr=GeneralizedLinearRegression(labelCol=target_col,family=\"gaussian\",maxIter=10,regParam=0.3)\n",
    "model = glr.fit(train_data)\n",
    "\n",
    "print(\"Coefficients: \", model.coefficients)\n",
    "print(\"Intercept: \", model.intercept)\n",
    "\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site with really good examples https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.2492365894055577,0.6650634453347885,0.0,-0.7006800013585207,0.0,0.0,0.0,0.0,0.0,-0.0838721302436192,0.0,-0.008883669460361569,0.9064825195671519,0.0]\n",
      "Intercept: 1.4865691775513472\n",
      "RMSE: 13.445672\n",
      "r2: 0.666233\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "(train_data, test_data) = model_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol=target_col, maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with Scaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.19159575926939018,0.4771867766725499,0.0,-0.4970160380872573,0.0,-0.0025395739719759466,0.0,0.0,0.0,-0.041840849953451206,0.0,-0.5196905110401441,18.609427045352216,0.0]\n",
      "Intercept: 1.4909126321537371\n",
      "RMSE: 13.600124\n",
      "r2: 0.660929\n",
      "CPU times: user 38.1 ms, sys: 16.2 ms, total: 54.3 ms\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "(train_data, test_data) = scaled_model_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol=target_col, maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
